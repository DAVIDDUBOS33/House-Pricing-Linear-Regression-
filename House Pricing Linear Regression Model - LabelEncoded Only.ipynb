{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# importer le dataset \n",
    "dataset = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column  3 has Nan\n",
      "Column  6 has Nan\n",
      "Column  25 has Nan\n",
      "Column  26 has Nan\n",
      "Column  30 has Nan\n",
      "Column  31 has Nan\n",
      "Column  32 has Nan\n",
      "Column  33 has Nan\n",
      "Column  35 has Nan\n",
      "Column  42 has Nan\n",
      "Column  57 has Nan\n",
      "Column  58 has Nan\n",
      "Column  59 has Nan\n",
      "Column  60 has Nan\n",
      "Column  63 has Nan\n",
      "Column  64 has Nan\n",
      "Column  72 has Nan\n",
      "Column  73 has Nan\n",
      "Column  74 has Nan\n"
     ]
    }
   ],
   "source": [
    "def isnan(dataframe, column):\n",
    "    for i in range(0, column):\n",
    "            if dataframe.iloc[:,i].isnull().any() == True:\n",
    "                print(\"Column \", i, \"has Nan\")\n",
    "                \n",
    "\n",
    "isnan(dataset, 81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, 81):\n",
    "    if type(dataset.iloc[0, i]) == str or np.isnan(dataset.iloc[0,i]):\n",
    "        if dataset.iloc[:,i].isnull().any() == True:\n",
    "            dataset.iloc[:, i] = dataset.iloc[:, i].replace(np.nan, \"None\", regex = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 1:-1].values\n",
    "Y = dataset.iloc[:, 80:81].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values=\"NaN\", strategy = \"median\", axis = 0)\n",
    "\n",
    "k = []\n",
    "for i in range(0, 79):\n",
    "    if type(X[:,i].any()) == int or type(X[:,i].any()) == float:\n",
    "        if X[:,i].sum() != X[:,i].sum():\n",
    "            k += [i]\n",
    "imputer.fit(X[:, k])\n",
    "X[:,k] = imputer.transform(X[:, k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "for i in range(0,79):\n",
    "    if type(X[0,i]) == str:        \n",
    "        X[:,i] = labelencoder.fit_transform(X[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SÃ©parer entre training set et test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "\n",
    "# linear Regression Model\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor_lr = LinearRegression()\n",
    "regressor_lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = regressor_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21639.041852371833"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_lr = []\n",
    "        \n",
    "for i in range (0, 292):\n",
    "    if y_test[i] - y_pred_lr[i] < 0:\n",
    "        accuracy_lr.append(y_pred_lr[i] - y_test[i])\n",
    "    else:\n",
    "        accuracy_lr.append(y_test[i] - y_pred_lr[i])\n",
    "\n",
    "\n",
    "accuracy_lr = np.asarray(accuracy_lr)\n",
    "accuracy_lr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.853</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.845</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   104.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 24 May 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:43:08</td>     <th>  Log-Likelihood:    </th> <td> -17143.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1460</td>      <th>  AIC:               </th> <td>3.444e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1382</td>      <th>  BIC:               </th> <td>3.486e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    77</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td> -108.5552</td> <td>   44.938</td> <td>   -2.416</td> <td> 0.016</td> <td> -196.710</td> <td>  -20.400</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td> -909.0317</td> <td> 1591.432</td> <td>   -0.571</td> <td> 0.568</td> <td>-4030.915</td> <td> 2212.852</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td> -152.7743</td> <td>   49.374</td> <td>   -3.094</td> <td> 0.002</td> <td> -249.631</td> <td>  -55.918</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.4081</td> <td>    0.105</td> <td>    3.873</td> <td> 0.000</td> <td>    0.201</td> <td>    0.615</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td> 2.883e+04</td> <td> 1.39e+04</td> <td>    2.077</td> <td> 0.038</td> <td> 1601.283</td> <td> 5.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td> 2590.9692</td> <td> 3720.521</td> <td>    0.696</td> <td> 0.486</td> <td>-4707.510</td> <td> 9889.448</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td> -869.7954</td> <td>  651.468</td> <td>   -1.335</td> <td> 0.182</td> <td>-2147.769</td> <td>  408.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td> 3034.8973</td> <td> 1328.441</td> <td>    2.285</td> <td> 0.022</td> <td>  428.919</td> <td> 5640.876</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>-5.491e+04</td> <td> 3.28e+04</td> <td>   -1.674</td> <td> 0.094</td> <td>-1.19e+05</td> <td> 9421.682</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>   14.2099</td> <td>  539.916</td> <td>    0.026</td> <td> 0.979</td> <td>-1044.933</td> <td> 1073.353</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td> 5516.6001</td> <td> 3783.656</td> <td>    1.458</td> <td> 0.145</td> <td>-1905.730</td> <td> 1.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>  392.7374</td> <td>  153.092</td> <td>    2.565</td> <td> 0.010</td> <td>   92.419</td> <td>  693.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td> -962.4650</td> <td> 1002.296</td> <td>   -0.960</td> <td> 0.337</td> <td>-2928.652</td> <td> 1003.722</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>-9432.1668</td> <td> 3317.134</td> <td>   -2.843</td> <td> 0.005</td> <td>-1.59e+04</td> <td>-2925.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>-3021.5370</td> <td> 1477.169</td> <td>   -2.045</td> <td> 0.041</td> <td>-5919.274</td> <td> -123.800</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>-1124.1883</td> <td>  648.526</td> <td>   -1.733</td> <td> 0.083</td> <td>-2396.390</td> <td>  148.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td> 1.089e+04</td> <td> 1172.124</td> <td>    9.291</td> <td> 0.000</td> <td> 8590.558</td> <td> 1.32e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td> 5251.6896</td> <td> 1034.767</td> <td>    5.075</td> <td> 0.000</td> <td> 3221.806</td> <td> 7281.574</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>  228.9851</td> <td>   77.929</td> <td>    2.938</td> <td> 0.003</td> <td>   76.113</td> <td>  381.857</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>   -9.8689</td> <td>   67.173</td> <td>   -0.147</td> <td> 0.883</td> <td> -141.641</td> <td>  121.903</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td> 1899.6671</td> <td> 1111.478</td> <td>    1.709</td> <td> 0.088</td> <td> -280.699</td> <td> 4080.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td> 5217.2312</td> <td> 1490.617</td> <td>    3.500</td> <td> 0.000</td> <td> 2293.114</td> <td> 8141.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>-1005.5320</td> <td>  515.591</td> <td>   -1.950</td> <td> 0.051</td> <td>-2016.957</td> <td>    5.893</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>  418.4983</td> <td>  466.272</td> <td>    0.898</td> <td> 0.370</td> <td> -496.178</td> <td> 1333.175</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td> 4509.8429</td> <td> 1576.993</td> <td>    2.860</td> <td> 0.004</td> <td> 1416.284</td> <td> 7603.401</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>   34.2845</td> <td>    6.022</td> <td>    5.693</td> <td> 0.000</td> <td>   22.471</td> <td>   46.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>-1.011e+04</td> <td> 1948.989</td> <td>   -5.188</td> <td> 0.000</td> <td>-1.39e+04</td> <td>-6288.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td>  708.7648</td> <td> 1239.323</td> <td>    0.572</td> <td> 0.567</td> <td>-1722.392</td> <td> 3139.922</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td> 1175.0623</td> <td> 1632.963</td> <td>    0.720</td> <td> 0.472</td> <td>-2028.292</td> <td> 4378.417</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td>-4173.0213</td> <td> 1017.025</td> <td>   -4.103</td> <td> 0.000</td> <td>-6168.102</td> <td>-2177.941</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td> 1344.1970</td> <td>  956.709</td> <td>    1.405</td> <td> 0.160</td> <td> -532.561</td> <td> 3220.955</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td>-3642.5538</td> <td>  873.179</td> <td>   -4.172</td> <td> 0.000</td> <td>-5355.453</td> <td>-1929.654</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td> -695.0594</td> <td>  511.465</td> <td>   -1.359</td> <td> 0.174</td> <td>-1698.392</td> <td>  308.273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td>    3.6657</td> <td>    2.880</td> <td>    1.273</td> <td> 0.203</td> <td>   -1.985</td> <td>    9.316</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td>  680.0041</td> <td> 1035.658</td> <td>    0.657</td> <td> 0.512</td> <td>-1351.627</td> <td> 2711.635</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td>    5.5010</td> <td>    5.745</td> <td>    0.958</td> <td> 0.338</td> <td>   -5.769</td> <td>   16.771</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td>   -4.1484</td> <td>    2.918</td> <td>   -1.422</td> <td> 0.155</td> <td>   -9.873</td> <td>    1.576</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td>    5.0208</td> <td>    3.359</td> <td>    1.495</td> <td> 0.135</td> <td>   -1.569</td> <td>   11.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td>-2148.4514</td> <td> 3163.288</td> <td>   -0.679</td> <td> 0.497</td> <td>-8353.817</td> <td> 4056.914</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td> -545.3867</td> <td>  606.624</td> <td>   -0.899</td> <td> 0.369</td> <td>-1735.391</td> <td>  644.618</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>   <td>  228.9945</td> <td> 4420.121</td> <td>    0.052</td> <td> 0.959</td> <td>-8441.878</td> <td> 8899.867</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>   <td> -373.5527</td> <td>  719.831</td> <td>   -0.519</td> <td> 0.604</td> <td>-1785.632</td> <td> 1038.527</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>   <td>   25.0296</td> <td>    5.856</td> <td>    4.274</td> <td> 0.000</td> <td>   13.541</td> <td>   36.518</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>   <td>   19.3259</td> <td>    5.375</td> <td>    3.596</td> <td> 0.000</td> <td>    8.782</td> <td>   29.869</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>   <td>  -21.0467</td> <td>   13.886</td> <td>   -1.516</td> <td> 0.130</td> <td>  -48.287</td> <td>    6.193</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>   <td>   23.3065</td> <td>    5.445</td> <td>    4.280</td> <td> 0.000</td> <td>   12.625</td> <td>   33.988</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>   <td> 6626.7631</td> <td> 2425.978</td> <td>    2.732</td> <td> 0.006</td> <td> 1867.765</td> <td> 1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>   <td>  -72.5133</td> <td> 3818.831</td> <td>   -0.019</td> <td> 0.985</td> <td>-7563.846</td> <td> 7418.820</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th>   <td> 3392.2309</td> <td> 2668.329</td> <td>    1.271</td> <td> 0.204</td> <td>-1842.182</td> <td> 8626.644</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th>   <td>   15.4711</td> <td> 2501.833</td> <td>    0.006</td> <td> 0.995</td> <td>-4892.329</td> <td> 4923.271</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th>   <td>-3667.8828</td> <td> 1643.787</td> <td>   -2.231</td> <td> 0.026</td> <td>-6892.471</td> <td> -443.295</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x52</th>   <td>-1.208e+04</td> <td> 4943.660</td> <td>   -2.443</td> <td> 0.015</td> <td>-2.18e+04</td> <td>-2380.672</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th>   <td>-8513.5054</td> <td> 1424.158</td> <td>   -5.978</td> <td> 0.000</td> <td>-1.13e+04</td> <td>-5719.761</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x54</th>   <td> 3822.5700</td> <td> 1161.413</td> <td>    3.291</td> <td> 0.001</td> <td> 1544.247</td> <td> 6100.893</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x55</th>   <td> 3750.4268</td> <td>  937.680</td> <td>    4.000</td> <td> 0.000</td> <td> 1910.997</td> <td> 5589.857</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x56</th>   <td> 4980.1913</td> <td> 1663.161</td> <td>    2.994</td> <td> 0.003</td> <td> 1717.598</td> <td> 8242.785</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x57</th>   <td>-1427.7395</td> <td>  781.128</td> <td>   -1.828</td> <td> 0.068</td> <td>-2960.064</td> <td>  104.585</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x58</th>   <td>  898.3902</td> <td>  602.413</td> <td>    1.491</td> <td> 0.136</td> <td> -283.352</td> <td> 2080.133</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x59</th>   <td>  -15.6789</td> <td>   66.646</td> <td>   -0.235</td> <td> 0.814</td> <td> -146.416</td> <td>  115.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60</th>   <td>-1781.7760</td> <td>  896.132</td> <td>   -1.988</td> <td> 0.047</td> <td>-3539.701</td> <td>  -23.851</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x61</th>   <td> 1.134e+04</td> <td> 2687.191</td> <td>    4.219</td> <td> 0.000</td> <td> 6066.775</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x62</th>   <td>   -1.7514</td> <td>    9.462</td> <td>   -0.185</td> <td> 0.853</td> <td>  -20.313</td> <td>   16.810</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x63</th>   <td>-1306.7484</td> <td> 1284.141</td> <td>   -1.018</td> <td> 0.309</td> <td>-3825.824</td> <td> 1212.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x64</th>   <td>   68.9904</td> <td> 1434.701</td> <td>    0.048</td> <td> 0.962</td> <td>-2745.437</td> <td> 2883.418</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x65</th>   <td> 1984.9175</td> <td> 2029.565</td> <td>    0.978</td> <td> 0.328</td> <td>-1996.443</td> <td> 5966.278</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x66</th>   <td>   22.1636</td> <td>    7.483</td> <td>    2.962</td> <td> 0.003</td> <td>    7.485</td> <td>   36.842</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x67</th>   <td>   -2.5913</td> <td>   14.151</td> <td>   -0.183</td> <td> 0.855</td> <td>  -30.351</td> <td>   25.169</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x68</th>   <td>   -2.1824</td> <td>   15.475</td> <td>   -0.141</td> <td> 0.888</td> <td>  -32.539</td> <td>   28.174</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x69</th>   <td>   31.1117</td> <td>   28.654</td> <td>    1.086</td> <td> 0.278</td> <td>  -25.099</td> <td>   87.322</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x70</th>   <td>   46.3250</td> <td>   15.875</td> <td>    2.918</td> <td> 0.004</td> <td>   15.183</td> <td>   77.467</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x71</th>   <td> -286.7032</td> <td>   45.883</td> <td>   -6.249</td> <td> 0.000</td> <td> -376.711</td> <td> -196.695</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x72</th>   <td>-8.886e+04</td> <td> 1.33e+04</td> <td>   -6.661</td> <td> 0.000</td> <td>-1.15e+05</td> <td>-6.27e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x73</th>   <td>   76.3690</td> <td>  843.280</td> <td>    0.091</td> <td> 0.928</td> <td>-1577.878</td> <td> 1730.616</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x74</th>   <td>-3197.3360</td> <td> 2384.770</td> <td>   -1.341</td> <td> 0.180</td> <td>-7875.497</td> <td> 1480.825</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x75</th>   <td>    0.0624</td> <td>    1.741</td> <td>    0.036</td> <td> 0.971</td> <td>   -3.354</td> <td>    3.479</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x76</th>   <td> -170.2215</td> <td>  314.508</td> <td>   -0.541</td> <td> 0.588</td> <td> -787.187</td> <td>  446.744</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x77</th>   <td> -936.0788</td> <td>  646.105</td> <td>   -1.449</td> <td> 0.148</td> <td>-2203.531</td> <td>  331.373</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x78</th>   <td> -697.0715</td> <td>  569.119</td> <td>   -1.225</td> <td> 0.221</td> <td>-1813.503</td> <td>  419.359</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x79</th>   <td> 3112.2262</td> <td>  818.398</td> <td>    3.803</td> <td> 0.000</td> <td> 1506.789</td> <td> 4717.664</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 1.746e+06</td> <td> 1.31e+06</td> <td>    1.336</td> <td> 0.182</td> <td>-8.17e+05</td> <td> 4.31e+06</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>616.020</td> <th>  Durbin-Watson:     </th> <td>   1.921</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>66306.584</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.974</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>35.957</td>  <th>  Cond. No.          </th> <td>1.13e+16</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.853\n",
       "Model:                            OLS   Adj. R-squared:                  0.845\n",
       "Method:                 Least Squares   F-statistic:                     104.3\n",
       "Date:                Thu, 24 May 2018   Prob (F-statistic):               0.00\n",
       "Time:                        13:43:08   Log-Likelihood:                -17143.\n",
       "No. Observations:                1460   AIC:                         3.444e+04\n",
       "Df Residuals:                    1382   BIC:                         3.486e+04\n",
       "Df Model:                          77                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1          -108.5552     44.938     -2.416      0.016    -196.710     -20.400\n",
       "x2          -909.0317   1591.432     -0.571      0.568   -4030.915    2212.852\n",
       "x3          -152.7743     49.374     -3.094      0.002    -249.631     -55.918\n",
       "x4             0.4081      0.105      3.873      0.000       0.201       0.615\n",
       "x5          2.883e+04   1.39e+04      2.077      0.038    1601.283    5.61e+04\n",
       "x6          2590.9692   3720.521      0.696      0.486   -4707.510    9889.448\n",
       "x7          -869.7954    651.468     -1.335      0.182   -2147.769     408.179\n",
       "x8          3034.8973   1328.441      2.285      0.022     428.919    5640.876\n",
       "x9         -5.491e+04   3.28e+04     -1.674      0.094   -1.19e+05    9421.682\n",
       "x10           14.2099    539.916      0.026      0.979   -1044.933    1073.353\n",
       "x11         5516.6001   3783.656      1.458      0.145   -1905.730    1.29e+04\n",
       "x12          392.7374    153.092      2.565      0.010      92.419     693.056\n",
       "x13         -962.4650   1002.296     -0.960      0.337   -2928.652    1003.722\n",
       "x14        -9432.1668   3317.134     -2.843      0.005   -1.59e+04   -2925.005\n",
       "x15        -3021.5370   1477.169     -2.045      0.041   -5919.274    -123.800\n",
       "x16        -1124.1883    648.526     -1.733      0.083   -2396.390     148.014\n",
       "x17         1.089e+04   1172.124      9.291      0.000    8590.558    1.32e+04\n",
       "x18         5251.6896   1034.767      5.075      0.000    3221.806    7281.574\n",
       "x19          228.9851     77.929      2.938      0.003      76.113     381.857\n",
       "x20           -9.8689     67.173     -0.147      0.883    -141.641     121.903\n",
       "x21         1899.6671   1111.478      1.709      0.088    -280.699    4080.033\n",
       "x22         5217.2312   1490.617      3.500      0.000    2293.114    8141.349\n",
       "x23        -1005.5320    515.591     -1.950      0.051   -2016.957       5.893\n",
       "x24          418.4983    466.272      0.898      0.370    -496.178    1333.175\n",
       "x25         4509.8429   1576.993      2.860      0.004    1416.284    7603.401\n",
       "x26           34.2845      6.022      5.693      0.000      22.471      46.098\n",
       "x27        -1.011e+04   1948.989     -5.188      0.000   -1.39e+04   -6288.177\n",
       "x28          708.7648   1239.323      0.572      0.567   -1722.392    3139.922\n",
       "x29         1175.0623   1632.963      0.720      0.472   -2028.292    4378.417\n",
       "x30        -4173.0213   1017.025     -4.103      0.000   -6168.102   -2177.941\n",
       "x31         1344.1970    956.709      1.405      0.160    -532.561    3220.955\n",
       "x32        -3642.5538    873.179     -4.172      0.000   -5355.453   -1929.654\n",
       "x33         -695.0594    511.465     -1.359      0.174   -1698.392     308.273\n",
       "x34            3.6657      2.880      1.273      0.203      -1.985       9.316\n",
       "x35          680.0041   1035.658      0.657      0.512   -1351.627    2711.635\n",
       "x36            5.5010      5.745      0.958      0.338      -5.769      16.771\n",
       "x37           -4.1484      2.918     -1.422      0.155      -9.873       1.576\n",
       "x38            5.0208      3.359      1.495      0.135      -1.569      11.611\n",
       "x39        -2148.4514   3163.288     -0.679      0.497   -8353.817    4056.914\n",
       "x40         -545.3867    606.624     -0.899      0.369   -1735.391     644.618\n",
       "x41          228.9945   4420.121      0.052      0.959   -8441.878    8899.867\n",
       "x42         -373.5527    719.831     -0.519      0.604   -1785.632    1038.527\n",
       "x43           25.0296      5.856      4.274      0.000      13.541      36.518\n",
       "x44           19.3259      5.375      3.596      0.000       8.782      29.869\n",
       "x45          -21.0467     13.886     -1.516      0.130     -48.287       6.193\n",
       "x46           23.3065      5.445      4.280      0.000      12.625      33.988\n",
       "x47         6626.7631   2425.978      2.732      0.006    1867.765    1.14e+04\n",
       "x48          -72.5133   3818.831     -0.019      0.985   -7563.846    7418.820\n",
       "x49         3392.2309   2668.329      1.271      0.204   -1842.182    8626.644\n",
       "x50           15.4711   2501.833      0.006      0.995   -4892.329    4923.271\n",
       "x51        -3667.8828   1643.787     -2.231      0.026   -6892.471    -443.295\n",
       "x52        -1.208e+04   4943.660     -2.443      0.015   -2.18e+04   -2380.672\n",
       "x53        -8513.5054   1424.158     -5.978      0.000   -1.13e+04   -5719.761\n",
       "x54         3822.5700   1161.413      3.291      0.001    1544.247    6100.893\n",
       "x55         3750.4268    937.680      4.000      0.000    1910.997    5589.857\n",
       "x56         4980.1913   1663.161      2.994      0.003    1717.598    8242.785\n",
       "x57        -1427.7395    781.128     -1.828      0.068   -2960.064     104.585\n",
       "x58          898.3902    602.413      1.491      0.136    -283.352    2080.133\n",
       "x59          -15.6789     66.646     -0.235      0.814    -146.416     115.059\n",
       "x60        -1781.7760    896.132     -1.988      0.047   -3539.701     -23.851\n",
       "x61         1.134e+04   2687.191      4.219      0.000    6066.775    1.66e+04\n",
       "x62           -1.7514      9.462     -0.185      0.853     -20.313      16.810\n",
       "x63        -1306.7484   1284.141     -1.018      0.309   -3825.824    1212.327\n",
       "x64           68.9904   1434.701      0.048      0.962   -2745.437    2883.418\n",
       "x65         1984.9175   2029.565      0.978      0.328   -1996.443    5966.278\n",
       "x66           22.1636      7.483      2.962      0.003       7.485      36.842\n",
       "x67           -2.5913     14.151     -0.183      0.855     -30.351      25.169\n",
       "x68           -2.1824     15.475     -0.141      0.888     -32.539      28.174\n",
       "x69           31.1117     28.654      1.086      0.278     -25.099      87.322\n",
       "x70           46.3250     15.875      2.918      0.004      15.183      77.467\n",
       "x71         -286.7032     45.883     -6.249      0.000    -376.711    -196.695\n",
       "x72        -8.886e+04   1.33e+04     -6.661      0.000   -1.15e+05   -6.27e+04\n",
       "x73           76.3690    843.280      0.091      0.928   -1577.878    1730.616\n",
       "x74        -3197.3360   2384.770     -1.341      0.180   -7875.497    1480.825\n",
       "x75            0.0624      1.741      0.036      0.971      -3.354       3.479\n",
       "x76         -170.2215    314.508     -0.541      0.588    -787.187     446.744\n",
       "x77         -936.0788    646.105     -1.449      0.148   -2203.531     331.373\n",
       "x78         -697.0715    569.119     -1.225      0.221   -1813.503     419.359\n",
       "x79         3112.2262    818.398      3.803      0.000    1506.789    4717.664\n",
       "const       1.746e+06   1.31e+06      1.336      0.182   -8.17e+05    4.31e+06\n",
       "==============================================================================\n",
       "Omnibus:                      616.020   Durbin-Watson:                   1.921\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            66306.584\n",
       "Skew:                          -0.974   Prob(JB):                         0.00\n",
       "Kurtosis:                      35.957   Cond. No.                     1.13e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 2.55e-21. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.astype(float)\n",
    "\n",
    "import statsmodels.formula.api as sm\n",
    "X = np.append(X, np.ones((1460,1)).astype(int), axis=1) \n",
    "regressor_OLS = sm.OLS(endog = Y, exog = X).fit()\n",
    "regressor_OLS.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "                \n",
    "for i in range(0, 80):\n",
    "    if type(test.iloc[0, i]) == str or np.isnan(test.iloc[0,i]):\n",
    "        if test.iloc[:,i].isnull().any() == True:\n",
    "            test.iloc[:, i] = test.iloc[:, i].replace(np.nan, \"None\", regex = True)\n",
    "                      \n",
    "\n",
    "kaggle = test.iloc[:, 1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = []\n",
    "for i in range(0, 79):\n",
    "    if type(kaggle[:,i].any()) == int or type(kaggle[:,i].any()) == float:\n",
    "        if kaggle[:,i].sum() != kaggle[:,i].sum():\n",
    "            k += [i]\n",
    "        \n",
    "imputer.fit(kaggle[:, k])\n",
    "kaggle[:,k] = imputer.transform(kaggle[:, k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,79):\n",
    "    if type(kaggle[0,i]) == str:        \n",
    "        kaggle[:,i] = labelencoder.fit_transform(kaggle[:,i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_kaggle = regressor_lr.predict(kaggle)\n",
    "y_pred_kaggle = pd.DataFrame(y_pred_kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_kaggle.to_csv(\"submissions_final.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
